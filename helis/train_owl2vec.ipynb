{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from nltk import word_tokenize\n",
    "from owl2vec_star.lib.RDF2Vec_Embed import get_rdf2vec_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeDict(dict):\n",
    "    def __getattr__(self, attr):\n",
    "        return self[attr]\n",
    "\n",
    "    def __setattr__(self, attr, value):\n",
    "        self[attr] = value\n",
    "\n",
    "# Usage\n",
    "ROOT_DIR = Path.cwd().parent / \"data\" / \"owl2vec\"\n",
    "FLAGS = AttributeDict()\n",
    "FLAGS['onto_file'] = str(ROOT_DIR / \"helis_v1.00.train.projection.ttl\")\n",
    "FLAGS['train_file'] = ROOT_DIR / \"train.csv\"\n",
    "FLAGS['valid_file'] = ROOT_DIR / \"valid.csv\"\n",
    "FLAGS['test_file'] = ROOT_DIR / \"test.csv\"\n",
    "FLAGS['class_file'] = ROOT_DIR / \"classes.txt\"\n",
    "FLAGS['individual_file'] = ROOT_DIR / \"individuals.txt\"\n",
    "FLAGS['inferred_class_file'] = ROOT_DIR / \"inferred_classes.txt\"\n",
    "FLAGS[\"embedsize\"] = 100\n",
    "\n",
    "FLAGS[\"URI_Doc\"] =\"yes\"\n",
    "FLAGS[\"Lit_Doc\"] =\"yes\"\n",
    "FLAGS[\"Mix_Doc\"] =\"yes\"\n",
    "FLAGS[\"Mix_Type\"] =\"random\"\n",
    "FLAGS[\"Embed_Out_URI\"] =\"yes\"\n",
    "FLAGS[\"Embed_Out_Words\"] =\"yes\"\n",
    "\n",
    "FLAGS[\"input_type\"] =\"concatenate\"\n",
    "FLAGS[\"walk_depth\"] = 4\n",
    "FLAGS[\"walker\"] =\"wl\"\n",
    "FLAGS[\"axiom_file\"] =ROOT_DIR / 'axioms.txt'\n",
    "FLAGS[\"annotation_file\"] =ROOT_DIR / 'annotations.txt'\n",
    "\n",
    "# addional flags for save the embeddings\n",
    "FLAGS[\"save_ind_path\"] = ROOT_DIR / \"Helis_individuals_e.csv\"\n",
    "FLAGS[\"save_class_path\"] = ROOT_DIR / \"Helis_classes_e.csv\"\n",
    "\n",
    "individuals = [line.strip() for line in open(FLAGS.individual_file).readlines()]\n",
    "classes = [line.strip() for line in open(FLAGS.class_file).readlines()]\n",
    "candidate_num = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed(model, instances):\n",
    "    def word_embeding(inst):\n",
    "        v = np.zeros(model.vector_size)\n",
    "        if inst in uri_label:\n",
    "            words = uri_label.get(inst)\n",
    "            n = 0\n",
    "            for word in words:\n",
    "                if word in model.wv.index_to_key:\n",
    "                    v += model.wv.get_vector(word)\n",
    "                    n += 1\n",
    "            return v / n if n > 0 else v\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    feature_vectors = []\n",
    "    for instance in instances:\n",
    "        if FLAGS.Embed_Out_Words.lower() == 'yes' and FLAGS.Embed_Out_URI.lower() == 'yes':\n",
    "            v_uri = model.wv.get_vector(instance) if instance in model.wv.index_to_key else np.zeros(model.vector_size)\n",
    "            v_word = word_embeding(inst=instance)\n",
    "            feature_vectors.append(np.concatenate((v_uri, v_word)))\n",
    "\n",
    "        elif FLAGS.Embed_Out_Words.lower() == 'no' and FLAGS.Embed_Out_URI.lower() == 'yes':\n",
    "            v_uri = model.wv.get_vector(instance) if instance in model.wv.index_to_key else np.zeros(model.vector_size)\n",
    "            feature_vectors.append(v_uri)\n",
    "\n",
    "        elif FLAGS.Embed_Out_Words.lower() == 'yes' and FLAGS.Embed_Out_URI.lower() == 'no':\n",
    "            v_word = word_embeding(inst=instance)\n",
    "            feature_vectors.append(v_word)\n",
    "\n",
    "        else:\n",
    "            print(\"Unknown embed out type\")\n",
    "            sys.exit(0)\n",
    "\n",
    "    return feature_vectors\n",
    "\n",
    "\n",
    "def pre_process_words(words):\n",
    "    text = ' '.join([re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', word, flags=re.MULTILINE) for word in words])\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "def URI_parse(uri):\n",
    "    \"\"\"Parse a URI: remove the prefix, parse the name part (Camel cases are plit)\"\"\"\n",
    "    uri = re.sub(\"http[a-zA-Z0-9:/._-]+#\", \"\", uri)\n",
    "    uri = uri.replace('_', ' ').replace('-', ' ').replace('.', ' ').replace('/', ' '). \\\n",
    "        replace('\"', ' ').replace(\"'\", ' ')\n",
    "    words = []\n",
    "    for item in uri.split():\n",
    "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', item)\n",
    "        for m in matches:\n",
    "            word = m.group(0)\n",
    "            if word.isalpha():\n",
    "                words.append(word.lower())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t1.Extract corpus and learning embedding ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\t\t1.Extract corpus and learning embedding ... \\n\")\n",
    "\n",
    "annotations = list()\n",
    "uri_label = dict()\n",
    "for line in open(FLAGS.annotation_file).readlines():\n",
    "    tmp = line.strip().split()\n",
    "    if tmp[1] not in ['http://www.w3.org/2000/01/rdf-schema#label', 'http://www.fbk.eu/ontologies/virtualcoach#id'] \\\n",
    "            and tmp[0] in classes + individuals:\n",
    "        annotations.append(tmp)\n",
    "    if tmp[1] == 'http://www.w3.org/2000/01/rdf-schema#label':\n",
    "        uri_label[tmp[0]] = pre_process_words(tmp[2:])\n",
    "for ent in individuals + classes:\n",
    "    if ent not in uri_label:\n",
    "        uri_label[ent] = URI_parse(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 5828824 walks for 20595 classes/individuals!\n",
      "Extracted 137512 axiom sentences\n"
     ]
    }
   ],
   "source": [
    "\n",
    "walk_sentences, axiom_sentences = list(), list()\n",
    "if FLAGS.URI_Doc.lower() == 'yes':\n",
    "    walks_ = get_rdf2vec_walks(onto_file=FLAGS.onto_file, walker_type=FLAGS.walker,\n",
    "                               walk_depth=FLAGS.walk_depth, classes=classes + individuals)\n",
    "    print('Extracted {} walks for {} classes/individuals!'.format(len(walks_), len(classes) + len(individuals)))\n",
    "    walk_sentences += [list(map(str, x)) for x in walks_]\n",
    "    for line in open(FLAGS.axiom_file).readlines():\n",
    "        axiom_sentence = [item for item in line.strip().split()]\n",
    "        axiom_sentences.append(axiom_sentence)\n",
    "    print('Extracted %d axiom sentences' % len(axiom_sentences))\n",
    "URI_Doc = walk_sentences + axiom_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 222 literal annotations\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Lit_Doc = list()\n",
    "if FLAGS.Lit_Doc.lower() == 'yes':\n",
    "    for annotation in annotations:\n",
    "        processed_words = pre_process_words(annotation[2:])\n",
    "        if len(processed_words) > 0:\n",
    "            Lit_Doc.append(uri_label[annotation[0]] + processed_words)\n",
    "    print('Extracted %d literal annotations' % len(Lit_Doc))\n",
    "\n",
    "    for sentence in walk_sentences:\n",
    "        lit_sentence = list()\n",
    "        for item in sentence:\n",
    "            if item in uri_label:\n",
    "                lit_sentence += uri_label[item]\n",
    "            elif item.startswith('http://www.w3.org'):\n",
    "                lit_sentence += [item.split('#')[1].lower()]\n",
    "            else:\n",
    "                lit_sentence += [item]\n",
    "        Lit_Doc.append(lit_sentence)\n",
    "\n",
    "    for sentence in axiom_sentences:\n",
    "        lit_sentence = list()\n",
    "        for item in sentence:\n",
    "            lit_sentence += uri_label[item] if item in uri_label else [item.lower()]\n",
    "        Lit_Doc.append(lit_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Mix_Doc = list()\n",
    "if FLAGS.Mix_Doc.lower() == 'yes':\n",
    "    for sentence in walk_sentences:\n",
    "        if FLAGS.Mix_Type.lower() == 'all':\n",
    "            for index in range(len(sentence)):\n",
    "                mix_sentence = list()\n",
    "                for i, item in enumerate(sentence):\n",
    "                    if i == index:\n",
    "                        mix_sentence += [item]\n",
    "                    else:\n",
    "                        if item in uri_label:\n",
    "                            mix_sentence += uri_label[item]\n",
    "                        elif item.startswith('http://www.w3.org'):\n",
    "                            mix_sentence += [item.split('#')[1].lower()]\n",
    "                        else:\n",
    "                            mix_sentence += [item]\n",
    "                Mix_Doc.append(mix_sentence)\n",
    "        elif FLAGS.Mix_Type.lower() == 'random':\n",
    "            random_index = random.randint(0, len(sentence) - 1)\n",
    "            mix_sentence = list()\n",
    "            for i, item in enumerate(sentence):\n",
    "                if i == random_index:\n",
    "                    mix_sentence += [item]\n",
    "                else:\n",
    "                    if item in uri_label:\n",
    "                        mix_sentence += uri_label[item]\n",
    "                    elif item.startswith('http://www.w3.org'):\n",
    "                        mix_sentence += [item.split('#')[1].lower()]\n",
    "                    else:\n",
    "                        mix_sentence += [item]\n",
    "            Mix_Doc.append(mix_sentence)\n",
    "\n",
    "    for sentence in axiom_sentences:\n",
    "        if FLAGS.Mix_Type.lower() == 'all':\n",
    "            for index in range(len(sentence)):\n",
    "                random_index = random.randint(0, len(sentence) - 1)\n",
    "                mix_sentence = list()\n",
    "                for i, item in enumerate(sentence):\n",
    "                    if i == random_index:\n",
    "                        mix_sentence += [item]\n",
    "                    else:\n",
    "                        mix_sentence += uri_label[item] if item in uri_label else [item.lower()]\n",
    "                Mix_Doc.append(mix_sentence)\n",
    "        elif FLAGS.Mix_Type.lower() == 'random':\n",
    "            random_index = random.randint(0, len(sentence) - 1)\n",
    "            mix_sentence = list()\n",
    "            for i, item in enumerate(sentence):\n",
    "                if i == random_index:\n",
    "                    mix_sentence += [item]\n",
    "                else:\n",
    "                    mix_sentence += uri_label[item] if item in uri_label else [item.lower()]\n",
    "            Mix_Doc.append(mix_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI_Doc: 5966336, Lit_Doc: 5966558, Mix_Doc: 5966336\n"
     ]
    }
   ],
   "source": [
    "print('URI_Doc: %d, Lit_Doc: %d, Mix_Doc: %d' % (len(URI_Doc), len(Lit_Doc), len(Mix_Doc)))\n",
    "all_doc = URI_Doc + Lit_Doc + Mix_Doc\n",
    "random.shuffle(all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = gensim.models.Word2Vec(all_doc, vector_size=FLAGS.embedsize, window=5, workers=multiprocessing.cpu_count(),\n",
    "                                    sg=1, epochs=10, negative=25, min_count=1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes_e = embed(model=model_, instances=classes)\n",
    "individuals_e = embed(model=model_, instances=individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(classes_e).to_csv(FLAGS.save_class_path, index=False, header=False)\n",
    "pd.DataFrame(individuals_e).to_csv(FLAGS.save_ind_path, index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
