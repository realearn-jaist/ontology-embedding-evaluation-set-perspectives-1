{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -u OWL2Vec_Plus.py --walker wl --walk_depth 4 --URI_Doc yes --Lit_Doc yes --Embed_Out_URI no --Embed_Out_Words yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle as pk\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from nltk import word_tokenize\n",
    "from owl2vec_star.lib.Evaluator import Evaluator\n",
    "from owl2vec_star.lib.RDF2Vec_Embed import get_rdf2vec_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeDict(dict):\n",
    "    def __getattr__(self, attr):\n",
    "        return self[attr]\n",
    "\n",
    "    def __setattr__(self, attr, value):\n",
    "        self[attr] = value\n",
    "\n",
    "# Usage\n",
    "ROOT_DIR = Path.cwd().parent / \"data\" / \"owl2vec\"\n",
    "FLAGS = AttributeDict()\n",
    "FLAGS['onto_file'] = str(ROOT_DIR / \"foodon-merged.train.owl\")\n",
    "FLAGS['train_file'] = ROOT_DIR / \"train.csv\"\n",
    "FLAGS['valid_file'] = ROOT_DIR / \"valid.csv\"\n",
    "FLAGS['test_file'] = ROOT_DIR / \"test.csv\"\n",
    "FLAGS['class_file'] = ROOT_DIR / \"classes.txt\"\n",
    "FLAGS['inferred_ancestor_file'] = ROOT_DIR / \"inferred_ancestors.txt\"\n",
    "FLAGS[\"embedsize\"] = 100\n",
    "\n",
    "FLAGS[\"URI_Doc\"] =\"yes\"\n",
    "FLAGS[\"Lit_Doc\"] =\"yes\"\n",
    "FLAGS[\"Mix_Doc\"] =\"yes\"\n",
    "FLAGS[\"Mix_Type\"] =\"random\"\n",
    "FLAGS[\"Embed_Out_URI\"] =\"yes\"\n",
    "FLAGS[\"Embed_Out_Words\"] =\"yes\"\n",
    "\n",
    "FLAGS[\"input_type\"] =\"concatenate\"\n",
    "FLAGS[\"walk_depth\"] = 4\n",
    "FLAGS[\"walker\"] =\"wl\"\n",
    "FLAGS[\"axiom_file\"] =ROOT_DIR / 'axioms.txt'\n",
    "FLAGS[\"annotation_file\"] =ROOT_DIR / 'annotations.txt'\n",
    "\n",
    "# addional flags for save the embeddings\n",
    "EMB_PATH = Path.cwd() / \"save_owl2vec_weights\" / \"owl2vec3.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "classes = [line.strip() for line in open(FLAGS.class_file).readlines()]\n",
    "candidate_num = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def URI_parse(uri):\n",
    "    \"\"\"Parse a URI: remove the prefix, parse the name part (Camel cases are plit)\"\"\"\n",
    "    uri = re.sub(\"http[a-zA-Z0-9:/._-]+#\", \"\", uri)\n",
    "    uri = uri.replace('_', ' ').replace('-', ' ').replace('.', ' ').replace('/', ' '). \\\n",
    "        replace('\"', ' ').replace(\"'\", ' ')\n",
    "    words = []\n",
    "    for item in uri.split():\n",
    "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', item)\n",
    "        for m in matches:\n",
    "            word = m.group(0)\n",
    "            if word.isalpha():\n",
    "                words.append(word.lower())\n",
    "    return words\n",
    "\n",
    "\n",
    "def embed(model, instances):\n",
    "\n",
    "    def word_embeding(inst):\n",
    "        v = np.zeros(model.vector_size)\n",
    "        if inst in uri_label:\n",
    "            words = uri_label.get(inst)\n",
    "            n = 0\n",
    "            for word in words:\n",
    "                if word in model.wv.index_to_key:\n",
    "                    v += model.wv.get_vector(word)\n",
    "                    n += 1\n",
    "            return v / n if n > 0 else v\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    feature_vectors = []\n",
    "    for instance in instances:\n",
    "        if FLAGS.Embed_Out_Words.lower() == 'yes' and FLAGS.Embed_Out_URI.lower() == 'yes':\n",
    "            v_uri = model.wv.get_vector(instance) if instance in model.wv.index_to_key else np.zeros(model.vector_size)\n",
    "            v_word = word_embeding(inst=instance)\n",
    "            feature_vectors.append(np.concatenate((v_uri, v_word)))\n",
    "\n",
    "        elif FLAGS.Embed_Out_Words.lower() == 'no' and FLAGS.Embed_Out_URI.lower() == 'yes':\n",
    "            v_uri = model.wv.get_vector(instance) if instance in model.wv.index_to_key else np.zeros(model.vector_size)\n",
    "            feature_vectors.append(v_uri)\n",
    "\n",
    "        elif FLAGS.Embed_Out_Words.lower() == 'yes' and FLAGS.Embed_Out_URI.lower() == 'no':\n",
    "            v_word = word_embeding(inst=instance)\n",
    "            feature_vectors.append(v_word)\n",
    "\n",
    "        else:\n",
    "            print(\"Unknown embed out type\")\n",
    "            sys.exit(0)\n",
    "\n",
    "    return feature_vectors\n",
    "\n",
    "\n",
    "def pre_process_words(words):\n",
    "    text = ' '.join([re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', word, flags=re.MULTILINE) for word in words])\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    return processed_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract corpus and learning embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uri_label = dict()\n",
    "annotations = list()\n",
    "for line in open(FLAGS.annotation_file, encoding=\"utf8\").readlines():\n",
    "    tmp = line.strip().split()\n",
    "    if tmp[1] == 'http://www.w3.org/2000/01/rdf-schema#label':\n",
    "        uri_label[tmp[0]] = pre_process_words(tmp[2:])\n",
    "    elif tmp[0] in classes:\n",
    "        annotations.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2218855 walks for 28182 classes!\n",
      "Extracted 34184 axiom sentences\n"
     ]
    }
   ],
   "source": [
    "\n",
    "walk_sentences, axiom_sentences = list(), list()\n",
    "if FLAGS.URI_Doc.lower() == 'yes':\n",
    "    walks_ = get_rdf2vec_walks(onto_file=FLAGS.onto_file, walker_type=FLAGS.walker,\n",
    "                               walk_depth=FLAGS.walk_depth, classes=classes)\n",
    "    print('Extracted {} walks for {} classes!'.format(len(walks_), len(classes)))\n",
    "    walk_sentences += [list(map(str, x)) for x in walks_]\n",
    "    for line in open(FLAGS.axiom_file).readlines():\n",
    "        axiom_sentence = [item for item in line.strip().split()]\n",
    "        axiom_sentences.append(axiom_sentence)\n",
    "    print('Extracted %d axiom sentences' % len(axiom_sentences))\n",
    "URI_Doc = walk_sentences + axiom_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 52249 literal annotations\n"
     ]
    }
   ],
   "source": [
    "Lit_Doc = list()\n",
    "if FLAGS.Lit_Doc.lower() == 'yes':\n",
    "    for annotation in annotations:\n",
    "        processed_words = pre_process_words(annotation[2:])\n",
    "        if len(processed_words) > 0:\n",
    "            Lit_Doc.append(uri_label[annotation[0]] + processed_words)\n",
    "    print('Extracted %d literal annotations' % len(Lit_Doc))\n",
    "\n",
    "    for sentence in walk_sentences:\n",
    "        lit_sentence = list()\n",
    "        for item in sentence:\n",
    "            if item in uri_label:\n",
    "                lit_sentence += uri_label[item]\n",
    "            elif item.startswith('http://www.w3.org'):\n",
    "                lit_sentence += [item.split('#')[1].lower()]\n",
    "            else:\n",
    "                lit_sentence += [item]\n",
    "        Lit_Doc.append(lit_sentence)\n",
    "\n",
    "    for sentence in axiom_sentences:\n",
    "        lit_sentence = list()\n",
    "        for item in sentence:\n",
    "            lit_sentence += uri_label[item] if item in uri_label else [item.lower()]\n",
    "        Lit_Doc.append(lit_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Mix_Doc = list()\n",
    "if FLAGS.Mix_Doc.lower() == 'yes':\n",
    "    for sentence in walk_sentences:\n",
    "        if FLAGS.Mix_Type.lower() == 'all':\n",
    "            for index in range(len(sentence)):\n",
    "                mix_sentence = list()\n",
    "                for i, item in enumerate(sentence):\n",
    "                    if i == index:\n",
    "                        mix_sentence += [item]\n",
    "                    else:\n",
    "                        if item in uri_label:\n",
    "                            mix_sentence += uri_label[item]\n",
    "                        elif item.startswith('http://www.w3.org'):\n",
    "                            mix_sentence += [item.split('#')[1].lower()]\n",
    "                        else:\n",
    "                            mix_sentence += [item]\n",
    "                Mix_Doc.append(mix_sentence)\n",
    "        elif FLAGS.Mix_Type.lower() == 'random':\n",
    "            random_index = random.randint(0, len(sentence)-1)\n",
    "            mix_sentence = list()\n",
    "            for i, item in enumerate(sentence):\n",
    "                if i == random_index:\n",
    "                    mix_sentence += [item]\n",
    "                else:\n",
    "                    if item in uri_label:\n",
    "                        mix_sentence += uri_label[item]\n",
    "                    elif item.startswith('http://www.w3.org'):\n",
    "                        mix_sentence += [item.split('#')[1].lower()]\n",
    "                    else:\n",
    "                        mix_sentence += [item]\n",
    "            Mix_Doc.append(mix_sentence)\n",
    "\n",
    "    for sentence in axiom_sentences:\n",
    "        if FLAGS.Mix_Type.lower() == 'all':\n",
    "            for index in range(len(sentence)):\n",
    "                random_index = random.randint(0, len(sentence) - 1)\n",
    "                mix_sentence = list()\n",
    "                for i, item in enumerate(sentence):\n",
    "                    if i == random_index:\n",
    "                        mix_sentence += [item]\n",
    "                    else:\n",
    "                        mix_sentence += uri_label[item] if item in uri_label else [item.lower()]\n",
    "                Mix_Doc.append(mix_sentence)\n",
    "        elif FLAGS.Mix_Type.lower() == 'random':\n",
    "            random_index = random.randint(0, len(sentence)-1)\n",
    "            mix_sentence = list()\n",
    "            for i, item in enumerate(sentence):\n",
    "                if i == random_index:\n",
    "                    mix_sentence += [item]\n",
    "                else:\n",
    "                    mix_sentence += uri_label[item] if item in uri_label else [item.lower()]\n",
    "            Mix_Doc.append(mix_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI_Doc: 2253039, Lit_Doc: 2305288, Mix_Doc: 2253039\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('URI_Doc: %d, Lit_Doc: %d, Mix_Doc: %d' % (len(URI_Doc), len(Lit_Doc), len(Mix_Doc)))\n",
    "all_doc = URI_Doc + Lit_Doc + Mix_Doc\n",
    "random.shuffle(all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = gensim.models.Word2Vec(all_doc, vector_size=FLAGS.embedsize, window=5, workers=multiprocessing.cpu_count(),\n",
    "                                    sg=1, epochs=10, negative=25, min_count=1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_e = embed(model=model_, instances=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMB_PATH.exists():\n",
    "    raise FileExistsError(f\"file {EMB_PATH} already exist, consider changing name to not confuse after\")\n",
    "\n",
    "save_obj = dict(\n",
    "    classes_e = classes_e,\n",
    "    model_config = dict(FLAGS)\n",
    ")\n",
    "with open(EMB_PATH, \"wb\") as file:\n",
    "    pk.dump(save_obj, file)\n",
    "# pd.DataFrame(classes_e).to_csv(FLAGS.save_path, index=False, header=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
